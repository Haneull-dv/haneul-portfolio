from transformers import TrainingArguments, Trainer
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import sys
import os

# utills í´ë” import
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
from utills.model_builder import ModelBuilder
from utills.data_loader import DataLoader

class TrainerService:
    def __init__(self, data_path="data/news_classifier_dataset.csv", model_name="klue/bert-base"):
        self.data_path = data_path
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.eval_dataset = None
    
    def validate_training_config(self, data_path: str, output_dir: str) -> dict:
        """
        í•™ìŠµ ì„¤ì • ê²€ì¦ ë¡œì§
        """
        validation_result = {
            "is_valid": True,
            "errors": []
        }
        
        # ë°ì´í„° íŒŒì¼ ê²€ì¦
        if not os.path.exists(data_path):
            validation_result["is_valid"] = False
            validation_result["errors"].append(f"ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        try:
            os.makedirs(output_dir, exist_ok=True)
        except Exception as e:
            validation_result["is_valid"] = False
            validation_result["errors"].append(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        
        return validation_result
        
    def setup_training(self):
        """í•™ìŠµ ì„¤ì • ì´ˆê¸°í™”"""
        # ë°ì´í„° ë¡œë“œ
        dataset, df = DataLoader.load_dataset(self.data_path)
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        class_weights_tensor = DataLoader.calculate_class_weights(df["label"])
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ìƒì„±
        self.model = ModelBuilder.create_model(self.model_name)
        self.tokenizer = ModelBuilder.create_tokenizer(self.model_name)
        self.model.set_class_weights(class_weights_tensor)
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        tokenized_dataset = DataLoader.preprocess_dataset(dataset, self.tokenizer)
        self.train_dataset, self.eval_dataset = DataLoader.split_dataset(tokenized_dataset)
        
        return True
    
    def compute_metrics(self, p):
        """í‰ê°€ ì§€í‘œ ê³„ì‚°"""
        preds = np.argmax(p.predictions, axis=1)
        labels = p.label_ids
        return {
            "accuracy": accuracy_score(labels, preds),
            "precision": precision_score(labels, preds),
            "recall": recall_score(labels, preds),
            "f1": f1_score(labels, preds),
        }
    
    def train_model(self, output_dir="./outputs", epochs=3, batch_size=8, learning_rate=2e-5):
        """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""
        training_args = TrainingArguments(
            output_dir=output_dir,
            learning_rate=learning_rate,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            num_train_epochs=epochs,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            logging_dir="./logs",
            logging_steps=10,
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            tokenizer=self.tokenizer,
            compute_metrics=self.compute_metrics,
        )
        
        # í•™ìŠµ ì‹¤í–‰
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        self.model.save_pretrained(f"{output_dir}/model")
        self.tokenizer.save_pretrained(f"{output_dir}/model")
        
        return trainer
    
    def run_full_training_pipeline(self, output_dir="./outputs", **kwargs):
        """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            print("ğŸ“‹ í•™ìŠµ ì„¤ì • ì´ˆê¸°í™” ì¤‘...")
            self.setup_training()
            
            print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
            trainer = self.train_model(output_dir=output_dir, **kwargs)
            
            print(f"âœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ {output_dir}/model ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return trainer
            
        except Exception as e:
            print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise e

    def execute_full_training(self, data_path: str, output_dir: str = "./outputs", 
                             model_name: str = "klue/bert-base", **kwargs):
        """
        ì „ì²´ í•™ìŠµ ì‹¤í–‰ ë¡œì§ (ê²€ì¦ í¬í•¨)
        ì»¨íŠ¸ë¡¤ëŸ¬ì—ì„œ í˜¸ì¶œí•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ
        """
        # 1. ì…ë ¥ ê²€ì¦
        if not data_path or not data_path.strip():
            raise ValueError("ë°ì´í„° íŒŒì¼ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if not output_dir:
            raise ValueError("ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # 2. ì„¤ì • ê²€ì¦
        validation = self.validate_training_config(data_path, output_dir)
        if not validation["is_valid"]:
            error_msg = "; ".join(validation["errors"])
            raise ValueError(f"ì„¤ì • ê²€ì¦ ì‹¤íŒ¨: {error_msg}")
        
        # 3. ì„œë¹„ìŠ¤ ì„¤ì • ì—…ë°ì´íŠ¸
        self.data_path = data_path
        self.model_name = model_name
        
        # 4. í•™ìŠµ ì‹¤í–‰
        trainer = self.run_full_training_pipeline(output_dir=output_dir, **kwargs)
        
        return trainer

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
trainer_service = TrainerService() 