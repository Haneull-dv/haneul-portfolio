torch>=2.0.0
transformers>=4.40.0
fastapi>=0.100.0
uvicorn[standard]>=0.20.0
pydantic>=2.0.0
# 추론용 의존성
bitsandbytes>=0.42.0
peft>=0.5.0
accelerate>=0.21.0
sentencepiece>=0.1.99
protobuf>=3.20.0
psutil>=5.9.0
# KoGPT2 특별 의존성
tokenizers>=0.15.0
# 추가 안정성
packaging>=21.0 