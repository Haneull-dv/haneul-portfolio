"""
QLoRA ëª¨ë¸ ë¡œë” ìœ í‹¸ë¦¬í‹°
ëª¨ë¸ ë¡œë”©, ì„¤ì •, ì €ì¥ ê´€ë ¨ ê¸°ëŠ¥
"""
import os
import gc
import torch
import logging
from typing import Tuple, Any
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    GPT2LMHeadModel,
    PreTrainedTokenizerFast,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)
from datetime import datetime

logger = logging.getLogger(__name__)

class ModelLoader:
    """QLoRA ëª¨ë¸ ë¡œë”"""
    
    def __init__(self):
        self.base_model_name = "skt/kogpt2-base-v2"  # KoGPT2 í•œêµ­ì–´ ìƒì„± ëª¨ë¸ (RTX 2080 í˜¸í™˜, í•œêµ­ì–´ ìš”ì•½ ìµœì í™”)
        self.output_dir = "./outputs"
        
    async def load_model_for_training(self) -> Tuple[Any, Any]:
        """í•™ìŠµìš© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            logger.info("Loading model and tokenizer for training...")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            gc.collect()
            
            # ì˜¤í”„ë¡œë“œ í´ë” ìƒì„±
            os.makedirs("./offload", exist_ok=True)
            
            # ì´ˆê¸° GPU ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê·¸
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3
                gpu_props = torch.cuda.get_device_properties(0)
                logger.info(f"GPU Memory: {gpu_allocated:.2f}GB / {gpu_memory:.2f}GB allocated")
                logger.info(f"GPU Compute Capability: {gpu_props.major}.{gpu_props.minor} (bf16 supported: {gpu_props.major >= 8 or (gpu_props.major == 7 and gpu_props.minor >= 5)})")
            
            # KoGPT2-base QLoRA ìµœì í™”ëœ 4bit ì–‘ìí™” ì„¤ì •
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,  # fp16 ì‚¬ìš©
                bnb_4bit_quant_storage=torch.uint8
            )
            
            # KoGPT2 ì „ìš© í† í¬ë‚˜ì´ì € ë¡œë”© (ì§ì ‘ ë°©ì‹)
            logger.info("Loading KoGPT2 tokenizer with direct method...")
            
            try:
                # 1ì°¨ ì‹œë„: GPT2Tokenizer ì§ì ‘ ì‚¬ìš© (ê°€ì¥ ì•ˆì „)
                from transformers import GPT2Tokenizer
                logger.info("Trying GPT2Tokenizer directly...")
                tokenizer = GPT2Tokenizer.from_pretrained(
                    self.base_model_name,
                    use_fast=False
                )
                logger.info("âœ… GPT2Tokenizer ë¡œë”© ì„±ê³µ")
                
            except Exception as e1:
                logger.warning(f"GPT2Tokenizer ì‹¤íŒ¨: {e1}")
                
                try:
                    # 2ì°¨ ì‹œë„: AutoTokenizer with explicit settings
                    logger.info("Trying AutoTokenizer with explicit settings...")
                    tokenizer = AutoTokenizer.from_pretrained(
                        self.base_model_name,
                        trust_remote_code=True,
                        use_fast=False,
                        tokenizer_type="gpt2"  # ëª…ì‹œì  tokenizer type ì§€ì •
                    )
                    logger.info("âœ… AutoTokenizer ë¡œë”© ì„±ê³µ")
                    
                except Exception as e2:
                    logger.warning(f"AutoTokenizerë„ ì‹¤íŒ¨: {e2}")
                    
                    try:
                        # 3ì°¨ ì‹œë„: ê¸°ë³¸ ì„¤ì •
                        logger.info("Trying basic AutoTokenizer...")
                        tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
                        logger.info("âœ… ê¸°ë³¸ AutoTokenizer ë¡œë”© ì„±ê³µ")
                        
                    except Exception as e3:
                        logger.error(f"ëª¨ë“  tokenizer ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {e3}")
                        raise e3
            
            # KoGPT2 í† í¬ë‚˜ì´ì € í•„ìˆ˜ ì„¤ì •
            logger.info("Configuring KoGPT2 tokenizer settings...")
            
            # 1. íŒ¨ë”© í† í° ì„¤ì • (vocab_size ë²”ìœ„ ë‚´ì—ì„œ)
            vocab_size = getattr(tokenizer, 'vocab_size', 51200)
            logger.info(f"í† í¬ë‚˜ì´ì € vocab_size: {vocab_size}")
            
            # eos_token_idê°€ vocab_sizeë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
            if tokenizer.eos_token_id >= vocab_size:
                # vocab_size ë²”ìœ„ ë‚´ì˜ ì•ˆì „í•œ í† í° ì‚¬ìš©
                safe_pad_token_id = vocab_size - 1  # ë§ˆì§€ë§‰ vocab í† í° ì‚¬ìš©
                tokenizer.pad_token_id = safe_pad_token_id
                try:
                    tokenizer.pad_token = tokenizer.convert_ids_to_tokens([safe_pad_token_id])[0]
                except:
                    tokenizer.pad_token = f"<pad_{safe_pad_token_id}>"
                logger.warning(f"eos_token_id ({tokenizer.eos_token_id}) >= vocab_size ({vocab_size}), ì•ˆì „í•œ pad_token ì‚¬ìš©: id={tokenizer.pad_token_id}")
            else:
                # ì •ìƒì ì¸ ê²½ìš°: eos_tokenì„ pad_tokenìœ¼ë¡œ ì‚¬ìš©
                if not hasattr(tokenizer, 'pad_token') or tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    tokenizer.pad_token_id = tokenizer.eos_token_id
                    logger.info(f"pad_token ì„¤ì •: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})")
            
            # 2. ìµœëŒ€ ê¸¸ì´ ì„¤ì •
            if not hasattr(tokenizer, 'model_max_length') or tokenizer.model_max_length > 512:
                tokenizer.model_max_length = 512
            
            # 3. GPT2 ê³„ì—´ íŒ¨ë”© ë°©í–¥ ì„¤ì • (í•„ìˆ˜!)
            tokenizer.padding_side = "right"  # GPT2ëŠ” ë°˜ë“œì‹œ right padding
            tokenizer.truncation_side = "left"  # ì…ë ¥ì´ ê¸¸ë©´ ì•ë¶€ë¶„ ìë¥´ê¸°
            
            # 4. GPT2 ê³„ì—´ ì¶”ê°€ ì•ˆì „ ì„¤ì •
            if hasattr(tokenizer, 'add_eos_token'):
                tokenizer.add_eos_token = True
            
            # 5. GPT2 tokenizer ê²€ì¦
            if tokenizer.pad_token != tokenizer.eos_token:
                logger.warning(f"GPT2 tokenizer pad_token({tokenizer.pad_token}) != eos_token({tokenizer.eos_token}), ì¬ì„¤ì • ì¤‘...")
                tokenizer.pad_token = tokenizer.eos_token
                tokenizer.pad_token_id = tokenizer.eos_token_id
            
            # ì„¤ì • í™•ì¸ ë° ë¡œê¹…
            vocab_size = getattr(tokenizer, 'vocab_size', 'Unknown')
            logger.info(f"âœ… KoGPT2 Tokenizer ì„¤ì • ì™„ë£Œ:")
            logger.info(f"   - vocab_size: {vocab_size}")
            logger.info(f"   - model_max_length: {tokenizer.model_max_length}")
            logger.info(f"   - pad_token: '{tokenizer.pad_token}' (id: {tokenizer.pad_token_id})")
            logger.info(f"   - eos_token: '{tokenizer.eos_token}' (id: {tokenizer.eos_token_id})")
            logger.info(f"   - padding_side: {tokenizer.padding_side}")
            
            # KoGPT2-base ëª¨ë¸ìš© device_map ì„¤ì • (RTX 2080 ìµœì í™”)
            device_map = "auto"  # KoGPT2-baseëŠ” 8GB GPUì— ì¶©ë¶„íˆ ë“¤ì–´ê°
            
            logger.info("Using auto device_map for KoGPT2-base (GPU-only allocation)")
            
            # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (KoGPT2 ìµœì í™”ëœ ì„¤ì •)
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_name,
                    quantization_config=bnb_config,
                    device_map=device_map,
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True,
                    max_memory={0: "6GB"},  # KoGPT2ëŠ” ë” ì‘ìœ¼ë¯€ë¡œ 6GBë¡œ ì œí•œ
                    use_cache=False,  # í•™ìŠµ ì‹œ cache ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
                    attn_implementation="eager"  # FlashAttention ì˜¤ë¥˜ ë°©ì§€
                )
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                # ëŒ€ì²´ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_name,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True
                )
            
            # QLoRA ì„¤ì • ì ìš©
            model = self._setup_lora(model)
            
            # gradient_checkpointing ë¹„í™œì„±í™” (QLoRA + offload ì¶©ëŒ ë°©ì§€)
            if hasattr(model, 'gradient_checkpointing_disable'):
                model.gradient_checkpointing_disable()
                logger.info("Gradient checkpointing disabled for QLoRA compatibility")
            
            logger.info("Model and tokenizer loaded successfully")
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise e
    
    def _setup_lora(self, model):
        """QLoRA ì„¤ì • ë° ì ìš©"""
        logger.info("Setting up QLoRA configuration")
        
        # ëª¨ë¸ì„ kbit í•™ìŠµìš©ìœ¼ë¡œ ì¤€ë¹„ (gradient checkpointing ì™„ì „ ë¹„í™œì„±í™”)
        model = prepare_model_for_kbit_training(
            model, 
            use_gradient_checkpointing=False,
            gradient_checkpointing_kwargs=None
        )
        
        # KoGPT2-base í•œêµ­ì–´ ìµœì í™”ëœ LoRA ì„¤ì •
        lora_config = LoraConfig(
            r=16,  # GPT2 ëª¨ë¸ì— ì í•©í•œ rank
            lora_alpha=32,  # alpha ì ì ˆíˆ ì¡°ì •
            target_modules=["c_attn", "c_proj"],  # GPT2 êµ¬ì¡°ì— ë§ëŠ” ëª¨ë“ˆ (attention layer)
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False
        )
        
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        
        logger.info("QLoRA configuration applied successfully")
        return model
    
    async def setup_trainer(self, model, tokenizer, train_dataset, config) -> Trainer:
        """RTX 2080 QLoRA ìµœì í™”ëœ Trainer ì„¤ì •"""
        try:
            logger.info("Setting up QLoRA-optimized trainer for RTX 2080...")
            
            # ëª¨ë¸ì˜ gradient checkpointing í™•ì‹¤íˆ ë¹„í™œì„±í™”
            if hasattr(model, 'gradient_checkpointing_disable'):
                model.gradient_checkpointing_disable()
                logger.info("Model gradient checkpointing disabled")
            
            # KoGPT2 RTX 2080 ìµœì í™”ëœ í•™ìŠµ ì¸ìˆ˜ ì„¤ì •
            training_args = TrainingArguments(
                output_dir=self.output_dir,
                per_device_train_batch_size=config.get("batch_size", 1),
                gradient_accumulation_steps=8,  # KoGPT2ëŠ” ì ì€ accumulationìœ¼ë¡œ ì•ˆì •ì 
                num_train_epochs=config.get("epochs", 15),
                learning_rate=config.get("learning_rate", 2e-4),
                
                # ì •ë°€ë„ ì„¤ì • (KoGPT2 + RTX 2080 ìµœì í™”)
                fp16=True,   # fp16 í™œì„±í™”
                bf16=False,  # bf16 ë¹„í™œì„±í™” 
                tf32=False,  # tf32 ë¹„í™œì„±í™”
                dataloader_pin_memory=False,  # ë©”ëª¨ë¦¬ ì•ˆì •ì„±
                
                # ì €ì¥ ë° ë¡œê¹…
                save_steps=200,
                logging_steps=10,
                save_strategy="steps",
                save_total_limit=2,
                
                # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
                warmup_ratio=0.03,
                lr_scheduler_type="cosine",
                
                # ì˜µí‹°ë§ˆì´ì € (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
                optim="paged_adamw_8bit",
                
                # ê¸°íƒ€ ì„¤ì •
                load_best_model_at_end=False,
                report_to=None,
                run_name=f"summarizer-qlora-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
                
                # ë°ì´í„°ë¡œë” ì„¤ì • (ì•ˆì •ì„± ìµœìš°ì„ )
                dataloader_num_workers=0,
                dataloader_drop_last=True,
                
                # ëª¨ë¸ ê´€ë ¨ ì„¤ì •
                remove_unused_columns=False,
                group_by_length=True,
                
                # Gradient ê´€ë ¨ ì„¤ì • (QLoRA ìµœì í™”)
                gradient_checkpointing=False,  # ì™„ì „íˆ ë¹„í™œì„±í™”
                max_grad_norm=1.0,
                
                # ë¶„ì‚° í•™ìŠµ ì„¤ì •
                ddp_find_unused_parameters=False,
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                prediction_loss_only=True,
                disable_tqdm=False,  # ì§„í–‰ ìƒí™© í‘œì‹œ
                
                # GPT2 ê³„ì—´ ì•ˆì •ì„± ì„¤ì •
                skip_memory_metrics=True,  # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê±´ë„ˆë›°ê¸°
                logging_nan_inf_filter=True,  # NaN/Inf ê°’ í•„í„°ë§
                length_column_name="length",  # ê¸¸ì´ ì»¬ëŸ¼ëª… ëª…ì‹œ
                ignore_data_skip=True,  # ë¬¸ì œ ë°ì´í„° ê±´ë„ˆë›°ê¸°
            )
            
            # GPT2 ê³„ì—´ ìµœì í™”ëœ ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
            data_collator = DataCollatorWithPadding(
                tokenizer=tokenizer,
                padding=True,
                max_length=512,
                pad_to_multiple_of=8,
                return_tensors="pt"
            )
            
            # ğŸ” ê°•í™”ëœ ë°ì´í„° ê²€ì¦ í•¨ìˆ˜ (device-side assert ë°©ì§€)
            def validate_dataset(dataset):
                """ë°ì´í„°ì…‹ ê°•í™” ê²€ì¦ ë° í•„í„°ë§ (device-side assert ë°©ì§€)"""
                logger.info("[VALIDATE] ë°ì´í„°ì…‹ ê²€ì¦ ì‹œì‘...")
                
                valid_count = 0
                error_stats = {
                    'missing_keys': 0,
                    'wrong_length': 0,
                    'invalid_input_ids': 0,
                    'invalid_labels': 0,
                    'length_mismatch': 0,
                    'type_errors': 0,
                    'no_learnable_tokens': 0,
                    'empty_sequence': 0
                }
                
                vocab_size = getattr(tokenizer, 'vocab_size', 51200)
                max_length = config.get('max_seq_length', 512)
                
                logger.info(f"[VALIDATE] ê²€ì¦ ê¸°ì¤€: vocab_size={vocab_size}, max_length={max_length}")
                
                for i, sample in enumerate(dataset):
                    try:
                        # 1. í•„ìˆ˜ í‚¤ ê²€ì¦
                        required_keys = ['input_ids', 'attention_mask', 'labels']
                        missing_keys = [key for key in required_keys if key not in sample]
                        if missing_keys:
                            error_stats['missing_keys'] += 1
                            if i < 3:  # ì²˜ìŒ 3ê°œë§Œ ë¡œê·¸
                                logger.warning(f"[ERROR] ìƒ˜í”Œ {i}: í•„ìˆ˜ í‚¤ ëˆ„ë½ {missing_keys}")
                            continue
                        
                        input_ids = sample['input_ids']
                        labels = sample['labels']
                        attention_mask = sample['attention_mask']
                        
                        # 2. íƒ€ì… ê²€ì¦
                        if not isinstance(input_ids, (list, torch.Tensor)):
                            error_stats['type_errors'] += 1
                            continue
                        
                        if isinstance(input_ids, torch.Tensor):
                            input_ids = input_ids.tolist()
                        if isinstance(labels, torch.Tensor):
                            labels = labels.tolist()
                        if isinstance(attention_mask, torch.Tensor):
                            attention_mask = attention_mask.tolist()
                        
                        # 3. ê¸¸ì´ ê²€ì¦
                        if len(input_ids) > max_length or len(input_ids) < 5:
                            error_stats['wrong_length'] += 1
                            if i < 3:
                                logger.warning(f"[ERROR] ìƒ˜í”Œ {i}: ë¶€ì ì ˆí•œ ê¸¸ì´ {len(input_ids)} (ë²”ìœ„: 5-{max_length})")
                            continue
                        
                        # 4. ì‹œí€€ìŠ¤ ê¸¸ì´ ì¼ì¹˜ ê²€ì¦
                        if len(input_ids) != len(labels) or len(input_ids) != len(attention_mask):
                            error_stats['length_mismatch'] += 1
                            if i < 3:
                                logger.warning(f"[ERROR] ìƒ˜í”Œ {i}: ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶ˆì¼ì¹˜ input={len(input_ids)}, labels={len(labels)}, mask={len(attention_mask)}")
                            continue
                        
                        # 5. ë¹ˆ ì‹œí€€ìŠ¤ ê²€ì¦
                        if all(token_id == tokenizer.pad_token_id for token_id in input_ids):
                            error_stats['empty_sequence'] += 1
                            continue
                        
                        # 6. ìŠ¤ë§ˆíŠ¸ vocab_size ê²€ì¦ (íŠ¹ë³„ í† í° ê³ ë ¤)
                        unk_token_id = getattr(tokenizer, 'unk_token_id', 0)
                        pad_token_id = getattr(tokenizer, 'pad_token_id', tokenizer.eos_token_id)
                        
                        # ì‹¤ì œ í—ˆìš© ê°€ëŠ¥í•œ í† í° ë²”ìœ„ í™•ì¥ (íŠ¹ë³„ í† í° ê³ ë ¤)
                        effective_vocab_size = max(vocab_size, tokenizer.eos_token_id + 1, pad_token_id + 1)
                        if unk_token_id is not None:
                            effective_vocab_size = max(effective_vocab_size, unk_token_id + 1)
                        
                        # ì²« ë²ˆì§¸ ìƒ˜í”Œì—ì„œë§Œ ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
                        if i == 0:
                            logger.info(f"[DEBUG] í† í° ë²”ìœ„ ì •ë³´:")
                            logger.info(f"   - vocab_size: {vocab_size}")
                            logger.info(f"   - effective_vocab_size: {effective_vocab_size}")
                            logger.info(f"   - unk_token_id: {unk_token_id}")
                            logger.info(f"   - pad_token_id: {pad_token_id}")
                            logger.info(f"   - eos_token_id: {tokenizer.eos_token_id}")
                        
                        # UNK í† í° ë¹„ìœ¨ ê²€ì¦ (ê·¹ë‹¨ì ì¸ ê²½ìš°ë§Œ í•„í„°ë§)
                        if unk_token_id is not None:
                            unk_ratio_input = input_ids.count(unk_token_id) / len(input_ids)
                            if unk_ratio_input > 0.9:  # 90% ì´ìƒì´ UNK í† í°
                                error_stats['invalid_input_ids'] += 1
                                if i < 3:
                                    logger.warning(f"[ERROR] ìƒ˜í”Œ {i}: UNK í† í° ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ìŒ: {unk_ratio_input:.2f}")
                                continue
                        
                        # 7. labels ê²€ì¦ - ìŠ¤ë§ˆíŠ¸ ë²”ìœ„ ì²´í¬
                        valid_labels = [label for label in labels if label != -100]
                        
                        if len(valid_labels) == 0:
                            error_stats['no_learnable_tokens'] += 1
                            if i < 3:
                                logger.warning(f"[ERROR] ìƒ˜í”Œ {i}: í•™ìŠµ ê°€ëŠ¥í•œ í† í° ì—†ìŒ (ëª¨ë“  labelsê°€ -100)")
                            continue
                        
                        # UNK í† í° ë¹„ìœ¨ ê²€ì¦ (labelsì—ì„œë„)
                        if unk_token_id is not None:
                            unk_ratio_labels = valid_labels.count(unk_token_id) / len(valid_labels)
                            if unk_ratio_labels > 0.9:  # 90% ì´ìƒì´ UNK í† í°
                                error_stats['invalid_labels'] += 1
                                if i < 3:
                                    logger.warning(f"[ERROR] ìƒ˜í”Œ {i}: labels UNK ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ìŒ: {unk_ratio_labels:.2f}")
                                continue
                        
                        # 8. ì¶”ê°€ ì•ˆì „ ê²€ì¦
                        # attention_maskëŠ” 0 ë˜ëŠ” 1ì´ì–´ì•¼ í•¨
                        invalid_mask = [mask for mask in attention_mask if mask not in [0, 1]]
                        if invalid_mask:
                            error_stats['type_errors'] += 1
                            if i < 3:
                                logger.warning(f"[ERROR] ìƒ˜í”Œ {i}: attention_mask ê°’ ì˜¤ë¥˜ {invalid_mask[:3]}...")
                            continue
                        
                        # âœ… ëª¨ë“  ê²€ì¦ í†µê³¼
                        valid_count += 1
                        
                        if i < 3:  # ì²˜ìŒ 3ê°œ ìƒ˜í”Œ ìƒì„¸ ì •ë³´
                            logger.info(f"[OK] ìƒ˜í”Œ {i} ê²€ì¦ í†µê³¼:")
                            logger.info(f"   - ê¸¸ì´: {len(input_ids)}")
                            logger.info(f"   - í•™ìŠµ ê°€ëŠ¥ í† í°: {len(valid_labels)}ê°œ")
                            logger.info(f"   - input_ids ë²”ìœ„: {min(input_ids)}-{max(input_ids)}")
                            logger.info(f"   - valid_labels ë²”ìœ„: {min(valid_labels)}-{max(valid_labels)}")
                        
                    except Exception as e:
                        error_stats['type_errors'] += 1
                        if i < 3:
                            logger.error(f"[ERROR] ìƒ˜í”Œ {i} ê²€ì¦ ì¤‘ ì˜ˆì™¸: {e}")
                        continue
                
                # ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸
                total_samples = len(dataset)
                invalid_count = total_samples - valid_count
                
                logger.info(f"[RESULT] ë°ì´í„°ì…‹ ê²€ì¦ ê²°ê³¼:")
                logger.info(f"   - ì „ì²´ ìƒ˜í”Œ: {total_samples}ê°œ")
                logger.info(f"   - ìœ íš¨ ìƒ˜í”Œ: {valid_count}ê°œ ({valid_count/total_samples*100:.1f}%)")
                logger.info(f"   - ë¬´íš¨ ìƒ˜í”Œ: {invalid_count}ê°œ ({invalid_count/total_samples*100:.1f}%)")
                
                if invalid_count > 0:
                    logger.warning(f"[WARNING] ì˜¤ë¥˜ ìœ í˜•ë³„ í†µê³„:")
                    for error_type, count in error_stats.items():
                        if count > 0:
                            logger.warning(f"   - {error_type}: {count}ê°œ")
                
                # ê²½ê³  ë° ì—ëŸ¬ ì¡°ê±´
                if valid_count == 0:
                    raise ValueError("[ERROR] ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤! ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                
                if valid_count < total_samples * 0.3:  # 30% ë¯¸ë§Œì´ ìœ íš¨í•˜ë©´ ê²½ê³ 
                    logger.warning(f"[WARNING] ìœ íš¨ ìƒ˜í”Œ ë¹„ìœ¨ì´ {valid_count/total_samples*100:.1f}%ë¡œ ë‚®ìŠµë‹ˆë‹¤.")
                
                logger.info("[OK] ë°ì´í„°ì…‹ ê²€ì¦ ì™„ë£Œ!")
                return valid_count, invalid_count
            
            # ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤í–‰
            validate_dataset(train_dataset)
            
            logger.info("Creating QLoRA trainer...")
            
            # GPT2 QLoRA ìµœì í™”ëœ Trainer ìƒì„±
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                data_collator=data_collator,
                tokenizer=tokenizer
            )
            
            # Trainer í›„ì²˜ë¦¬ ê²€ì¦
            logger.info("Verifying trainer configuration...")
            
            # í† í¬ë‚˜ì´ì € ì„¤ì • ì¬í™•ì¸
            assert trainer.tokenizer.pad_token is not None, "Tokenizer pad_token is None"
            assert trainer.tokenizer.pad_token_id is not None, "Tokenizer pad_token_id is None"
            assert trainer.tokenizer.padding_side == "right", f"Wrong padding_side: {trainer.tokenizer.padding_side}"
            
            # ë°ì´í„° ì½œë ˆì´í„° í™•ì¸
            assert hasattr(trainer.data_collator, 'tokenizer'), "Data collator missing tokenizer"
            
            logger.info("âœ… Trainer configuration verified successfully")
            
            # ì¶”ê°€ ì•ˆì „ì¥ì¹˜: í•™ìŠµ ì „ gradient ìƒíƒœ í™•ì¸
            logger.info("Verifying gradient setup...")
            for name, param in model.named_parameters():
                if param.requires_grad:
                    logger.debug(f"Trainable parameter: {name} on device {param.device}")
                    break  # ì²« ë²ˆì§¸ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ í™•ì¸
            
            logger.info("QLoRA trainer setup completed successfully")
            
            return trainer
            
        except Exception as e:
            logger.error(f"Failed to setup trainer: {str(e)}")
            raise e
    
    async def save_trained_model(self, trainer, tokenizer):
        """í•™ìŠµëœ ëª¨ë¸ ì €ì¥"""
        try:
            logger.info("Saving trained model...")
            
            # ëª¨ë¸ ì €ì¥
            trainer.save_model()
            tokenizer.save_pretrained(self.output_dir)
            
            logger.info(f"Model saved to {self.output_dir}")
            
        except Exception as e:
            logger.error(f"Failed to save model: {str(e)}")
            raise e 