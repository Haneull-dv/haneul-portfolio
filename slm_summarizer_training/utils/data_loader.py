"""
ë°ì´í„° ë¡œë” ìœ í‹¸ë¦¬í‹°
CSV ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ê¸°ëŠ¥
"""
import os
import pandas as pd
import logging
from typing import Any
from datasets import Dataset

logger = logging.getLogger(__name__)

class DataLoader:
    """ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.dataset_path = "./data/final_input_output_dataset_filtered.csv"
        
    async def load_training_dataset(self, tokenizer, config) -> Dataset:
        """í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            logger.info(f"Loading dataset from {self.dataset_path}")
            
            # CSV íŒŒì¼ ë¡œë“œ
            if not os.path.exists(self.dataset_path):
                raise FileNotFoundError(f"Dataset not found: {self.dataset_path}")
            
            df = pd.read_csv(self.dataset_path)
            logger.info(f"Loaded {len(df)} samples")
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            processed_data = self._preprocess_data(df, tokenizer, config)
            
            # Dataset ê°ì²´ ìƒì„±
            dataset = Dataset.from_dict(processed_data)
            
            logger.info("Dataset loaded and preprocessed successfully")
            return dataset
            
        except Exception as e:
            logger.error(f"Failed to load dataset: {str(e)}")
            raise e
    
    def _preprocess_data(self, df, tokenizer, config):
        """ë°ì´í„° ì „ì²˜ë¦¬ - Causal LMìš©"""
        try:
            max_seq_length = config.get("max_seq_length", 512)
            
            input_ids_list = []
            attention_mask_list = []
            labels_list = []
            
            for _, row in df.iterrows():
                # ì…ë ¥ í”„ë¡¬í”„íŠ¸ì™€ ì¶œë ¥ ë¶„ë¦¬
                input_prompt = self._create_input_prompt(str(row['input']))
                target_output = str(row['output'])
                
                # ë¹ˆ ë°ì´í„° ì²´í¬
                if not input_prompt.strip() or not target_output.strip():
                    logger.warning(f"Skipping empty data at index {row.name}")
                    continue
                
                # ì…ë ¥ ë¶€ë¶„ í† í¬ë‚˜ì´ì§• (KoGPT2 ì•ˆì „ ì²˜ë¦¬)
                try:
                    # ê¸°ë³¸ í† í¬ë‚˜ì´ì§• ì‹œë„
                    input_encoding = tokenizer(
                        input_prompt,
                        truncation=False,
                        padding=False,
                        add_special_tokens=True,
                        return_tensors=None
                    )
                    # input_ids ê²€ì¦
                    if not isinstance(input_encoding.get("input_ids"), list) or len(input_encoding["input_ids"]) == 0:
                        raise ValueError("Invalid input_ids")
                        
                except Exception as e:
                    logger.warning(f"Input encoding ì‹¤íŒ¨ ({e}), ë‹¨ìˆœ ë°©ë²• ì‹œë„...")
                    try:
                        # ë” ë‹¨ìˆœí•œ ë°©ë²• ì‹œë„
                        input_ids = tokenizer.encode(input_prompt, add_special_tokens=True)
                        input_encoding = {"input_ids": input_ids}
                    except Exception as e2:
                        logger.error(f"Input encoding ì™„ì „ ì‹¤íŒ¨: {e2}")
                        continue
                
                # ì¶œë ¥ ë¶€ë¶„ í† í¬ë‚˜ì´ì§• (special token ì—†ì´)
                try:
                    # ê¸°ë³¸ í† í¬ë‚˜ì´ì§• ì‹œë„
                    output_encoding = tokenizer(
                        target_output,
                        truncation=False,
                        padding=False,
                        add_special_tokens=False,
                        return_tensors=None
                    )
                    # output_ids ê²€ì¦
                    if not isinstance(output_encoding.get("input_ids"), list) or len(output_encoding["input_ids"]) == 0:
                        raise ValueError("Invalid output_ids")
                        
                except Exception as e:
                    logger.warning(f"Output encoding ì‹¤íŒ¨ ({e}), ë‹¨ìˆœ ë°©ë²• ì‹œë„...")
                    try:
                        # ë” ë‹¨ìˆœí•œ ë°©ë²• ì‹œë„
                        output_ids = tokenizer.encode(target_output, add_special_tokens=False)
                        output_encoding = {"input_ids": output_ids}
                    except Exception as e2:
                        logger.error(f"Output encoding ì™„ì „ ì‹¤íŒ¨: {e2}")
                        continue
                
                # ì „ì²´ ì‹œí€€ìŠ¤ êµ¬ì„± (KoGPT2 ì•ˆì „ ì²˜ë¦¬)
                try:
                    input_ids = input_encoding["input_ids"] + output_encoding["input_ids"] + [tokenizer.eos_token_id]
                except Exception as e:
                    logger.warning(f"ì‹œí€€ìŠ¤ êµ¬ì„± ì‹¤íŒ¨: {e}")
                    continue
                
                # ê¸¸ì´ ì œí•œ ë° ê²€ì¦
                if len(input_ids) > max_seq_length:
                    input_ids = input_ids[:max_seq_length]
                elif len(input_ids) < 10:  # ë„ˆë¬´ ì§§ì€ ì‹œí€€ìŠ¤ ì œì™¸
                    logger.warning(f"ì‹œí€€ìŠ¤ê°€ ë„ˆë¬´ ì§§ìŒ: {len(input_ids)} tokens")
                    continue
                
                # attention_mask ìƒì„±
                attention_mask = [1] * len(input_ids)
                
                # labels ìƒì„± (ì…ë ¥ ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹, ì¶œë ¥ ë¶€ë¶„ë§Œ í•™ìŠµ)
                try:
                    labels = [-100] * len(input_encoding["input_ids"]) + output_encoding["input_ids"] + [tokenizer.eos_token_id]
                    if len(labels) > max_seq_length:
                        labels = labels[:max_seq_length]
                except Exception as e:
                    logger.warning(f"Labels ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
                
                # íŒ¨ë”© (KoGPT2 ì•ˆì „ ì²˜ë¦¬)
                padding_length = max_seq_length - len(input_ids)
                if padding_length > 0:
                    # pad_token_idê°€ ì—†ëŠ” ê²½ìš° eos_token_id ì‚¬ìš©
                    pad_id = getattr(tokenizer, 'pad_token_id', tokenizer.eos_token_id)
                    if pad_id is None:
                        pad_id = tokenizer.eos_token_id
                    
                    input_ids.extend([pad_id] * padding_length)
                    attention_mask.extend([0] * padding_length)
                    labels.extend([-100] * padding_length)
                
                # ğŸ” ê°•í™”ëœ ìµœì¢… ê²€ì¦ (device-side assert ë°©ì§€)
                try:
                    # 1. ê¸¸ì´ ì¼ì¹˜ ê²€ì¦
                    if len(input_ids) != max_seq_length or len(attention_mask) != max_seq_length or len(labels) != max_seq_length:
                        logger.warning(f"[ERROR] ê¸¸ì´ ë¶ˆì¼ì¹˜: input_ids={len(input_ids)}, attention_mask={len(attention_mask)}, labels={len(labels)}")
                        continue
                    
                    # 2. í† í¬ë‚˜ì´ì € ì •ë³´ í™•ì¸ ë° ìŠ¤ë§ˆíŠ¸ vocab_size ê²€ì¦
                    vocab_size = getattr(tokenizer, 'vocab_size', 51200)
                    unk_token_id = getattr(tokenizer, 'unk_token_id', 0)  # UNK í† í° ID
                    pad_token_id = getattr(tokenizer, 'pad_token_id', tokenizer.eos_token_id)
                    
                    # ë””ë²„ê·¸: í† í° ë²”ìœ„ í™•ì¸ (ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ)
                    if len(input_ids_list) == 0:  # ì²« ë²ˆì§¸ ìƒ˜í”Œ
                        logger.info(f"[DEBUG] í† í¬ë‚˜ì´ì € ì •ë³´:")
                        logger.info(f"   - vocab_size: {vocab_size}")
                        logger.info(f"   - unk_token_id: {unk_token_id}")
                        logger.info(f"   - pad_token_id: {pad_token_id}")
                        logger.info(f"   - eos_token_id: {tokenizer.eos_token_id}")
                        logger.info(f"   - ìƒ˜í”Œ input_ids ë²”ìœ„: {min(input_ids)} ~ {max(input_ids)}")
                        logger.info(f"   - ìƒ˜í”Œ labels ë²”ìœ„: {min([l for l in labels if l != -100])} ~ {max([l for l in labels if l != -100])}")
                        
                        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ vocab_size ê³„ì‚° (íŠ¹ë³„ í† í° ê³ ë ¤)
                        special_tokens = set([
                            getattr(tokenizer, 'unk_token_id', None),
                            getattr(tokenizer, 'pad_token_id', None),
                            getattr(tokenizer, 'eos_token_id', None),
                            getattr(tokenizer, 'bos_token_id', None),
                            getattr(tokenizer, 'cls_token_id', None),
                            getattr(tokenizer, 'sep_token_id', None),
                            getattr(tokenizer, 'mask_token_id', None)
                        ])
                        special_tokens.discard(None)
                        max_special_token = max(special_tokens) if special_tokens else 0
                        logger.info(f"   - íŠ¹ë³„ í† í° ìµœëŒ€ê°’: {max_special_token}")
                        logger.info(f"   - íŠ¹ë³„ í† í° ì§‘í•©: {sorted(special_tokens)}")
                    
                    # ì•ˆì „í•œ í† í° ë²”ìœ„ ì²˜ë¦¬ (vocab_size ê¸°ì¤€)
                    max_valid_token_id = vocab_size - 1  # 0 ~ vocab_size-1ì´ ìœ íš¨í•œ ë²”ìœ„
                    
                    # input_idsì—ì„œ ë²”ìœ„ ì´ˆê³¼ í† í°ì„ max_valid_token_idë¡œ í´ë¦¬í•‘
                    original_input_ids = input_ids[:]
                    input_ids = []
                    input_fixes = 0
                    
                    for idx in original_input_ids:
                        if idx >= vocab_size or idx < 0:
                            input_ids.append(max_valid_token_id)  # vocab_size-1ë¡œ í´ë¦¬í•‘
                            input_fixes += 1
                        else:
                            input_ids.append(idx)
                    
                    # labelsì—ì„œ ë²”ìœ„ ì´ˆê³¼ í† í° ì²˜ë¦¬ (-100ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
                    original_labels = labels[:]
                    labels = []
                    label_fixes = 0
                    
                    for label in original_labels:
                        if label == -100:  # ignore indexëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                            labels.append(label)
                        elif label >= vocab_size or label < 0:
                            labels.append(max_valid_token_id)  # vocab_size-1ë¡œ í´ë¦¬í•‘
                            label_fixes += 1
                        else:
                            labels.append(label)
                    
                    # ìˆ˜ì • í†µê³„ ë¡œê·¸ (ì²˜ìŒ 3ê°œë§Œ)
                    if (input_fixes > 0 or label_fixes > 0) and len(input_ids_list) < 3:
                        logger.info(f"[FIX] í† í° ë²”ìœ„ ìˆ˜ì • (ìƒ˜í”Œ {len(input_ids_list)}):")
                        logger.info(f"   - vocab_size: {vocab_size}")
                        logger.info(f"   - max_valid_token_id: {max_valid_token_id}")
                        logger.info(f"   - input_fixes: {input_fixes}/{len(original_input_ids)}")
                        logger.info(f"   - label_fixes: {label_fixes}/{len([l for l in original_labels if l != -100])}")
                    
                    # ê·¹ë‹¨ì ì¸ ê²½ìš°ë§Œ í•„í„°ë§ (ëŒ€ë¶€ë¶„ì´ í´ë¦¬í•‘ëœ í† í°ì¸ ê²½ìš°)
                    clipped_ratio_input = input_ids.count(max_valid_token_id) / len(input_ids)
                    valid_labels = [l for l in labels if l != -100]
                    clipped_ratio_labels = (valid_labels.count(max_valid_token_id) / len(valid_labels)) if valid_labels else 0
                    
                    if clipped_ratio_input > 0.9:  # 90% ì´ìƒì´ í´ë¦¬í•‘ëœ í† í°
                        logger.warning(f"[ERROR] inputì— í´ë¦¬í•‘ëœ í† í°ì´ ë„ˆë¬´ ë§ìŒ: {clipped_ratio_input:.2f}")
                        continue
                    
                    if clipped_ratio_labels > 0.9:  # 90% ì´ìƒì´ í´ë¦¬í•‘ëœ í† í°
                        logger.warning(f"[ERROR] labelsì— í´ë¦¬í•‘ëœ í† í°ì´ ë„ˆë¬´ ë§ìŒ: {clipped_ratio_labels:.2f}")
                        continue
                    
                    # 3. ë°ì´í„° íƒ€ì… ê²€ì¦
                    if not all(isinstance(x, int) for x in input_ids):
                        logger.warning(f"[ERROR] input_ids íƒ€ì… ì˜¤ë¥˜: {[type(x) for x in input_ids[:5]]}")
                        continue
                    
                    if not all(isinstance(x, int) for x in labels):
                        logger.warning(f"[ERROR] labels íƒ€ì… ì˜¤ë¥˜: {[type(x) for x in labels[:5]]}")
                        continue
                    
                    # 4. í•™ìŠµ ê°€ëŠ¥í•œ í† í° ê²€ì¦
                    learnable_tokens = [label for label in labels if label != -100]
                    if len(learnable_tokens) == 0:
                        logger.warning(f"[ERROR] í•™ìŠµ ê°€ëŠ¥í•œ í† í°ì´ ì—†ìŒ (ëª¨ë“  labelsê°€ -100)")
                        continue
                    
                    if len(learnable_tokens) < 3:
                        logger.warning(f"[ERROR] í•™ìŠµ ê°€ëŠ¥í•œ í† í°ì´ ë„ˆë¬´ ì ìŒ: {len(learnable_tokens)}ê°œ")
                        continue
                    
                    # 5. íŒ¨ë”© í† í° ë¹„ìœ¨ ê²€ì¦
                    pad_token_id = getattr(tokenizer, 'pad_token_id', tokenizer.eos_token_id)
                    pad_ratio = input_ids.count(pad_token_id) / len(input_ids)
                    if pad_ratio > 0.8:  # 80% ì´ìƒì´ íŒ¨ë”©ì´ë©´ ì œì™¸
                        logger.warning(f"[ERROR] íŒ¨ë”© ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ìŒ: {pad_ratio:.2f}")
                        continue
                    
                    # ëª¨ë“  ê²€ì¦ í†µê³¼
                    logger.debug(f"[OK] ìƒ˜í”Œ ê²€ì¦ í†µê³¼: input_len={len(input_ids)}, learnable_tokens={len(learnable_tokens)}, pad_ratio={pad_ratio:.2f}")
                    
                except Exception as e:
                    logger.error(f"[ERROR] ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
                
                input_ids_list.append(input_ids)
                attention_mask_list.append(attention_mask)
                labels_list.append(labels)
            
            # ìµœì¢… ë°ì´í„°ì…‹ í†µê³„
            total_original = len(df)
            total_processed = len(input_ids_list)
            filter_rate = (total_original - total_processed) / total_original * 100
            
            logger.info(f"[RESULT] ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ:")
            logger.info(f"   - ì›ë³¸ ìƒ˜í”Œ: {total_original}ê°œ")
            logger.info(f"   - ì²˜ë¦¬ëœ ìƒ˜í”Œ: {total_processed}ê°œ")
            logger.info(f"   - í•„í„°ë§ëœ ìƒ˜í”Œ: {total_original - total_processed}ê°œ ({filter_rate:.1f}%)")
            
            if total_processed == 0:
                raise ValueError("[ERROR] ì²˜ë¦¬ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")
            
            if total_processed < 10:
                logger.warning(f"[WARNING] ì²˜ë¦¬ëœ ìƒ˜í”Œì´ ë§¤ìš° ì ìŠµë‹ˆë‹¤ ({total_processed}ê°œ). í•™ìŠµ í’ˆì§ˆì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # ìµœì¢… ê²€ì¦ í†µê³„
            final_dataset = {
                "input_ids": input_ids_list,
                "attention_mask": attention_mask_list,
                "labels": labels_list
            }
            
            # ìƒ˜í”Œ ë°ì´í„° ê²€ì¦
            if len(input_ids_list) > 0:
                sample_input = input_ids_list[0]
                sample_labels = labels_list[0]
                vocab_size = getattr(tokenizer, 'vocab_size', 51200)
                
                logger.info(f"[SAMPLE] ìƒ˜í”Œ ê²€ì¦:")
                logger.info(f"   - ìƒ˜í”Œ ê¸¸ì´: {len(sample_input)}")
                logger.info(f"   - vocab_size: {vocab_size}")
                logger.info(f"   - ìµœëŒ€ input_id: {max(sample_input)}")
                logger.info(f"   - ìµœëŒ€ valid_label: {max([l for l in sample_labels if l != -100], default=-100)}")
                logger.info(f"   - í•™ìŠµ ê°€ëŠ¥ í† í°: {len([l for l in sample_labels if l != -100])}ê°œ")
            
            return final_dataset
            
        except Exception as e:
            logger.error(f"Failed to preprocess data: {str(e)}")
            raise e
    
    def _create_input_prompt(self, input_text: str) -> str:
        """ì…ë ¥ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì¶œë ¥ ë¶€ë¶„ ì œì™¸)"""
        return f"""ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì…ë ¥: {input_text}

ìš”ì•½:""" 