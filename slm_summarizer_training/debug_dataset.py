#!/usr/bin/env python3
"""
ë°ì´í„°ì…‹ ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸ - device-side assert ì˜¤ë¥˜ í•´ê²°ìš©
ë°ì´í„°ì…‹ì˜ ë¬¸ì œì ì„ ì°¾ì•„ ìˆ˜ì •í•˜ëŠ” ìœ í‹¸ë¦¬í‹°
"""
import pandas as pd
import json
import os
import sys
from transformers import AutoTokenizer, GPT2Tokenizer
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DatasetDebugger:
    """ë°ì´í„°ì…‹ ë¬¸ì œ ì§„ë‹¨ ë° ìˆ˜ì • í´ë˜ìŠ¤"""
    
    def __init__(self, config_path="config.json", data_path="./data/final_input_output_dataset_filtered.csv"):
        self.config_path = config_path
        self.data_path = data_path
        self.tokenizer = None
        self.config = self._load_config()
        
    def _load_config(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {
                "model_name": "skt/kogpt2-base-v2",
                "max_seq_length": 512
            }
    
    def _load_tokenizer(self):
        """í† í¬ë‚˜ì´ì € ë¡œë“œ (3ë‹¨ê³„ fallback)"""
        model_name = self.config.get("model_name", "skt/kogpt2-base-v2")
        
        try:
            # 1ë‹¨ê³„: GPT2Tokenizer ì§ì ‘ ì‚¬ìš©
            logger.info(f"GPT2Tokenizerë¡œ {model_name} ë¡œë”© ì‹œë„...")
            tokenizer = GPT2Tokenizer.from_pretrained(model_name)
            
        except Exception as e1:
            logger.warning(f"GPT2Tokenizer ì‹¤íŒ¨: {e1}")
            try:
                # 2ë‹¨ê³„: AutoTokenizer ì‚¬ìš©
                logger.info(f"AutoTokenizerë¡œ {model_name} ë¡œë”© ì‹œë„...")
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                
            except Exception as e2:
                logger.warning(f"AutoTokenizer ì‹¤íŒ¨: {e2}")
                try:
                    # 3ë‹¨ê³„: use_fast=Falseë¡œ ì¬ì‹œë„
                    logger.info(f"AutoTokenizer (use_fast=False)ë¡œ {model_name} ë¡œë”© ì‹œë„...")
                    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
                    
                except Exception as e3:
                    logger.error(f"ëª¨ë“  í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e3}")
                    raise e3
        
        # í† í¬ë‚˜ì´ì € ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        
        # íŒ¨ë”© ë°©í–¥ ì„¤ì •
        tokenizer.padding_side = "right"
        
        logger.info(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ:")
        logger.info(f"   - ëª¨ë¸: {model_name}")
        logger.info(f"   - vocab_size: {tokenizer.vocab_size}")
        logger.info(f"   - pad_token: '{tokenizer.pad_token}' (id: {tokenizer.pad_token_id})")
        logger.info(f"   - eos_token: '{tokenizer.eos_token}' (id: {tokenizer.eos_token_id})")
        
        return tokenizer
    
    def analyze_dataset(self):
        """ë°ì´í„°ì…‹ ë¶„ì„"""
        print("ğŸ” ë°ì´í„°ì…‹ ë¶„ì„ ì‹œì‘...")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = self._load_tokenizer()
        
        # ë°ì´í„° ë¡œë“œ
        if not os.path.exists(self.data_path):
            print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_path}")
            return
        
        df = pd.read_csv(self.data_path)
        print(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ ìƒ˜í”Œ")
        
        # ë¶„ì„ í†µê³„
        stats = {
            'total_samples': len(df),
            'valid_samples': 0,
            'errors': {
                'missing_columns': 0,
                'empty_text': 0,
                'too_long': 0,
                'too_short': 0,
                'tokenization_error': 0,
                'invalid_tokens': 0,
                'invalid_labels': 0
            },
            'token_stats': {
                'min_length': float('inf'),
                'max_length': 0,
                'avg_length': 0,
                'total_tokens': 0
            }
        }
        
        vocab_size = self.tokenizer.vocab_size
        max_length = self.config.get('max_seq_length', 512)
        
        print(f"ğŸ”§ ë¶„ì„ ê¸°ì¤€:")
        print(f"   - vocab_size: {vocab_size}")
        print(f"   - max_length: {max_length}")
        print(f"   - í•„ìˆ˜ ì»¬ëŸ¼: input, output")
        print()
        
        problem_samples = []
        
        for idx, row in df.iterrows():
            try:
                # 1. í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
                if 'input' not in row or 'output' not in row:
                    stats['errors']['missing_columns'] += 1
                    problem_samples.append((idx, 'missing_columns', 'input ë˜ëŠ” output ì»¬ëŸ¼ ì—†ìŒ'))
                    continue
                
                input_text = str(row['input']).strip()
                output_text = str(row['output']).strip()
                
                # 2. ë¹ˆ í…ìŠ¤íŠ¸ í™•ì¸
                if not input_text or not output_text or input_text == 'nan' or output_text == 'nan':
                    stats['errors']['empty_text'] += 1
                    problem_samples.append((idx, 'empty_text', f'ë¹ˆ í…ìŠ¤íŠ¸: input="{input_text[:20]}...", output="{output_text[:20]}..."'))
                    continue
                
                # 3. í† í°í™” í…ŒìŠ¤íŠ¸
                try:
                    # ì…ë ¥ í† í°í™”
                    input_tokens = self.tokenizer.encode(input_text, add_special_tokens=True)
                    output_tokens = self.tokenizer.encode(output_text, add_special_tokens=False)
                    
                    # ê¸¸ì´ í™•ì¸
                    total_length = len(input_tokens) + len(output_tokens)
                    
                    if total_length > max_length:
                        stats['errors']['too_long'] += 1
                        problem_samples.append((idx, 'too_long', f'í† í° ê¸¸ì´ ì´ˆê³¼: {total_length} > {max_length}'))
                        continue
                    
                    if total_length < 10:
                        stats['errors']['too_short'] += 1
                        problem_samples.append((idx, 'too_short', f'í† í° ê¸¸ì´ ë¶€ì¡±: {total_length} < 10'))
                        continue
                    
                    # 4. í† í° ID ë²”ìœ„ í™•ì¸
                    all_tokens = input_tokens + output_tokens
                    invalid_tokens = [t for t in all_tokens if t >= vocab_size or t < 0]
                    
                    if invalid_tokens:
                        stats['errors']['invalid_tokens'] += 1
                        problem_samples.append((idx, 'invalid_tokens', f'ë²”ìœ„ ì´ˆê³¼ í† í°: {invalid_tokens[:3]}... (vocab_size: {vocab_size})'))
                        continue
                    
                    # 5. labels ìƒì„± í…ŒìŠ¤íŠ¸
                    combined_text = input_text + self.tokenizer.eos_token + output_text
                    encoded = self.tokenizer(
                        combined_text,
                        max_length=max_length,
                        padding="max_length",
                        truncation=True,
                        return_tensors="pt"
                    )
                    
                    input_ids = encoded["input_ids"][0].tolist()
                    
                    # labels ìƒì„± (ì…ë ¥ ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹)
                    labels = [-100] * len(input_tokens) + output_tokens
                    if len(labels) < max_length:
                        labels.extend([-100] * (max_length - len(labels)))
                    elif len(labels) > max_length:
                        labels = labels[:max_length]
                    
                    # labels ë²”ìœ„ í™•ì¸
                    valid_labels = [l for l in labels if l != -100]
                    invalid_labels = [l for l in valid_labels if l >= vocab_size or l < 0]
                    
                    if invalid_labels:
                        stats['errors']['invalid_labels'] += 1
                        problem_samples.append((idx, 'invalid_labels', f'ì˜ëª»ëœ labels: {invalid_labels[:3]}...'))
                        continue
                    
                    # âœ… ìœ íš¨í•œ ìƒ˜í”Œ
                    stats['valid_samples'] += 1
                    
                    # í† í° í†µê³„ ì—…ë°ì´íŠ¸
                    length = len([l for l in labels if l != -100])
                    stats['token_stats']['min_length'] = min(stats['token_stats']['min_length'], length)
                    stats['token_stats']['max_length'] = max(stats['token_stats']['max_length'], length)
                    stats['token_stats']['total_tokens'] += length
                    
                except Exception as e:
                    stats['errors']['tokenization_error'] += 1
                    problem_samples.append((idx, 'tokenization_error', f'í† í°í™” ì˜¤ë¥˜: {str(e)[:50]}...'))
                    continue
                    
            except Exception as e:
                stats['errors']['tokenization_error'] += 1
                problem_samples.append((idx, 'general_error', f'ì¼ë°˜ ì˜¤ë¥˜: {str(e)[:50]}...'))
                continue
        
        # ğŸ“Š ê²°ê³¼ ì¶œë ¥
        print("ğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"   - ì „ì²´ ìƒ˜í”Œ: {stats['total_samples']}ê°œ")
        print(f"   - ìœ íš¨ ìƒ˜í”Œ: {stats['valid_samples']}ê°œ ({stats['valid_samples']/stats['total_samples']*100:.1f}%)")
        print(f"   - ë¬¸ì œ ìƒ˜í”Œ: {len(problem_samples)}ê°œ ({len(problem_samples)/stats['total_samples']*100:.1f}%)")
        print()
        
        print("ğŸ”§ ì˜¤ë¥˜ ìœ í˜•ë³„ í†µê³„:")
        for error_type, count in stats['errors'].items():
            if count > 0:
                print(f"   - {error_type}: {count}ê°œ")
        print()
        
        if stats['valid_samples'] > 0:
            avg_length = stats['token_stats']['total_tokens'] / stats['valid_samples']
            print("ğŸ“ í† í° ê¸¸ì´ í†µê³„ (ìœ íš¨ ìƒ˜í”Œ):")
            print(f"   - ìµœì†Œ ê¸¸ì´: {stats['token_stats']['min_length']}")
            print(f"   - ìµœëŒ€ ê¸¸ì´: {stats['token_stats']['max_length']}")
            print(f"   - í‰ê·  ê¸¸ì´: {avg_length:.1f}")
            print()
        
        # ë¬¸ì œ ìƒ˜í”Œ ìƒì„¸ ì¶œë ¥ (ì²˜ìŒ 10ê°œ)
        if problem_samples:
            print("âŒ ë¬¸ì œ ìƒ˜í”Œ ìƒì„¸ (ì²˜ìŒ 10ê°œ):")
            for i, (idx, error_type, detail) in enumerate(problem_samples[:10]):
                print(f"   {i+1}. ìƒ˜í”Œ {idx}: [{error_type}] {detail}")
            
            if len(problem_samples) > 10:
                print(f"   ... ë° {len(problem_samples) - 10}ê°œ ë”")
            print()
        
        return stats, problem_samples
    
    def fix_dataset(self, output_path=None):
        """ë°ì´í„°ì…‹ ë¬¸ì œ ìˆ˜ì •"""
        print("ğŸ”§ ë°ì´í„°ì…‹ ìˆ˜ì • ì‹œì‘...")
        
        if output_path is None:
            base_name = os.path.splitext(self.data_path)[0]
            output_path = f"{base_name}_fixed.csv"
        
        # ë¶„ì„ ì‹¤í–‰
        stats, problem_samples = self.analyze_dataset()
        
        if stats['valid_samples'] == 0:
            print("âŒ ìˆ˜ì • ê°€ëŠ¥í•œ ìœ íš¨ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë°ì´í„° ë¡œë“œ ë° ìˆ˜ì •
        df = pd.read_csv(self.data_path)
        problem_indices = {idx for idx, _, _ in problem_samples}
        
        # ìœ íš¨í•œ ìƒ˜í”Œë§Œ í•„í„°ë§
        fixed_df = df.drop(index=problem_indices).reset_index(drop=True)
        
        print(f"ğŸ”§ ìˆ˜ì • ì™„ë£Œ:")
        print(f"   - ì›ë³¸ ìƒ˜í”Œ: {len(df)}ê°œ")
        print(f"   - ì œê±°ëœ ìƒ˜í”Œ: {len(problem_indices)}ê°œ")
        print(f"   - ìˆ˜ì •ëœ ìƒ˜í”Œ: {len(fixed_df)}ê°œ")
        print(f"   - ì €ì¥ ê²½ë¡œ: {output_path}")
        
        # ìˆ˜ì •ëœ ë°ì´í„° ì €ì¥
        fixed_df.to_csv(output_path, index=False, encoding='utf-8')
        print("âœ… ìˆ˜ì •ëœ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ!")
        
        return output_path

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë°ì´í„°ì…‹ ë””ë²„ê¹… ë„êµ¬")
    parser.add_argument("--data", default="./data/final_input_output_dataset_filtered.csv", help="ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--config", default="config.json", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--fix", action="store_true", help="ë¬¸ì œ ìˆ˜ì • ëª¨ë“œ")
    parser.add_argument("--output", help="ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    debugger = DatasetDebugger(args.config, args.data)
    
    if args.fix:
        debugger.fix_dataset(args.output)
    else:
        debugger.analyze_dataset()

if __name__ == "__main__":
    print("ğŸ” ë°ì´í„°ì…‹ ë””ë²„ê¹… ë„êµ¬")
    print("ì‚¬ìš©ë²•:")
    print("  ë¶„ì„ë§Œ: python debug_dataset.py")
    print("  ìˆ˜ì •: python debug_dataset.py --fix")
    print()
    
    main() 