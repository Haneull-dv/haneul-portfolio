#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ìš”ì•½ ëª¨ë¸ ì§ì ‘ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
API ì„œë²„ ì—†ì´ ë°”ë¡œ QLoRA í•™ìŠµì„ ì‹¤í–‰
"""
import sys
import os
import asyncio
import logging
import argparse
from datetime import datetime

# ğŸ”§ CUDA ë””ë²„ê¹… ì„¤ì • (device-side assert ì¶”ì )
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["TORCH_USE_CUDA_DSA"] = "1"  # Device-side assertion í™œì„±í™”
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # tokenizer ê²½ê³  ì œê±°

# GPU ë©”ëª¨ë¦¬ ë””ë²„ê¹… (ì„ íƒì‚¬í•­)
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

print("ğŸ”§ CUDA ë””ë²„ê¹… ëª¨ë“œ í™œì„±í™”:")
print("   - CUDA_LAUNCH_BLOCKING=1 (ë™ê¸° ì‹¤í–‰)")
print("   - TORCH_USE_CUDA_DSA=1 (device assertion)")
print("   - TOKENIZERS_PARALLELISM=false")
print()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# utils í´ë” import
from utils.model_loader import ModelLoader
from utils.data_loader import DataLoader

# ë¡œê¹… ì„¤ì • (Windows CP949 í˜¸í™˜)
import sys
import io

# Windowsì—ì„œ UTF-8 ì¶œë ¥ì„ ìœ„í•œ ì„¤ì •
if sys.platform.startswith('win'):
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class DirectTrainer:
    """ì§ì ‘ í•™ìŠµ ì‹¤í–‰ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model_loader = ModelLoader()
        self.data_loader = DataLoader()
    
    async def run_training(self, config):
        """í•™ìŠµ ì‹¤í–‰"""
        try:
            print("=" * 60)
            print("ğŸ¯ ë‰´ìŠ¤ ìš”ì•½ ëª¨ë¸ QLoRA í•™ìŠµ ì‹œì‘")
            print("=" * 60)
            
            # GPU ì •ë³´ ì¶œë ¥
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"ğŸ–¥ï¸  GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            else:
                print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ í•™ìŠµ (ë§¤ìš° ëŠë¦¼)")
            
            print(f"ğŸ“Š í•™ìŠµ ì„¤ì •:")
            print(f"   - ì—í¬í¬: {config['epochs']}")
            print(f"   - ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
            print(f"   - í•™ìŠµë¥ : {config['learning_rate']}")
            print(f"   - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {config['max_seq_length']}")
            print()
            
            # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
            print("ğŸ”„ 1ë‹¨ê³„: ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©...")
            model, tokenizer = await self.model_loader.load_model_for_training()
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
            # 2. ë°ì´í„°ì…‹ ë¡œë“œ
            print("ğŸ”„ 2ë‹¨ê³„: ë°ì´í„°ì…‹ ë¡œë”©...")
            train_dataset = await self.data_loader.load_training_dataset(tokenizer, config)
            print("âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ")
            
            # 3. í•™ìŠµ ì„¤ì •
            print("ğŸ”„ 3ë‹¨ê³„: í•™ìŠµ ì„¤ì •...")
            trainer = await self.model_loader.setup_trainer(
                model, tokenizer, train_dataset, config
            )
            print("âœ… í•™ìŠµ ì„¤ì • ì™„ë£Œ")
            
            # 4. í•™ìŠµ ì‹¤í–‰
            print("ğŸ”„ 4ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
            try:
                total_steps = len(trainer.get_train_dataloader()) * config['epochs']
                print(f"ğŸ“ˆ ì´ ìŠ¤í… ìˆ˜: {total_steps}")
            except:
                print("ğŸ“ˆ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            print()
            
            # ğŸ” í•™ìŠµ ì „ ìµœì¢… ì•ˆì „ ê²€ì¦
            logger.info("ğŸ” í•™ìŠµ ì „ ìµœì¢… ì•ˆì „ ê²€ì¦...")
            
            try:
                # í† í¬ë‚˜ì´ì € ìƒíƒœ ê²€ì¦
                assert tokenizer.pad_token is not None, "âŒ pad_tokenì´ Noneì…ë‹ˆë‹¤"
                assert tokenizer.pad_token_id is not None, "âŒ pad_token_idê°€ Noneì…ë‹ˆë‹¤"
                print(f"âœ… í† í¬ë‚˜ì´ì € ê²€ì¦: pad_token='{tokenizer.pad_token}', pad_token_id={tokenizer.pad_token_id}")
                
                # ëª¨ë¸ ìƒíƒœ ê²€ì¦
                print(f"âœ… ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}")
                print(f"âœ… ëª¨ë¸ dtype: {next(model.parameters()).dtype}")
                
                # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                total_params = sum(p.numel() for p in model.parameters())
                print(f"âœ… í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)")
                
                # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3
                    gpu_reserved = torch.cuda.memory_reserved(0) / 1024**3
                    print(f"âœ… GPU ë©”ëª¨ë¦¬: {gpu_allocated:.1f}GB í• ë‹¹ / {gpu_reserved:.1f}GB ì˜ˆì•½ / {gpu_memory:.1f}GB ì „ì²´")
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ê²½ê³ 
                    if gpu_allocated > gpu_memory * 0.85:
                        print(f"âš ï¸  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {gpu_allocated/gpu_memory*100:.1f}%")
                
            except Exception as e:
                print(f"âŒ í•™ìŠµ ì „ ê²€ì¦ ì‹¤íŒ¨: {e}")
                raise e
            
            print("\nğŸš€ QLoRA í•™ìŠµ ì‹œì‘...")
            print("   â€» CUDA_LAUNCH_BLOCKING=1 í™œì„±í™”ë¡œ ì˜¤ë¥˜ ìœ„ì¹˜ ì¶”ì  ê°€ëŠ¥")
            print("   â€» device-side assert ë°œìƒì‹œ ì •í™•í•œ ì˜¤ë¥˜ ìœ„ì¹˜ê°€ í‘œì‹œë©ë‹ˆë‹¤")
            print()
            
            # í•™ìŠµ ì‹œì‘ (ê°•í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬)
            try:
                train_result = trainer.train()
                print("âœ… í•™ìŠµ ì™„ë£Œ!")
                
            except RuntimeError as e:
                error_msg = str(e).lower()
                if "device-side assert" in error_msg or "assertion" in error_msg:
                    print("\nâŒ CUDA/device-side assertion ì˜¤ë¥˜ ë°œìƒ!")
                    print("ğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•:")
                    print("   1. ìœ„ ë°ì´í„° ê²€ì¦ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                    print("   2. vocab_size ì´ˆê³¼ í† í°ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
                    print("   3. labelsì— -100 ì™¸ì˜ ì´ìƒê°’ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
                    print("   4. input_ids/labels ê¸¸ì´ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
                    print("   5. ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì„ ì¬ê²€í† í•˜ì„¸ìš”")
                    print(f"\nğŸ› ì›ë³¸ ì˜¤ë¥˜ ë©”ì‹œì§€:")
                    print(f"   {e}")
                elif "out of memory" in error_msg or "cuda out of memory" in error_msg:
                    print("\nâŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜!")
                    print("ğŸ”§ í•´ê²° ë°©ë²•:")
                    print("   1. batch_sizeë¥¼ 1ë¡œ ì¤„ì´ì„¸ìš”")
                    print("   2. gradient_accumulation_stepsë¥¼ ëŠ˜ë¦¬ì„¸ìš”")
                    print("   3. max_seq_lengthë¥¼ ì¤„ì´ì„¸ìš” (ì˜ˆ: 256)")
                    print(f"\nğŸ› ì›ë³¸ ì˜¤ë¥˜: {e}")
                else:
                    print(f"\nâŒ CUDA ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
                raise e
            except Exception as e:
                print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                logger.error(f"í•™ìŠµ ì˜¤ë¥˜: {e}", exc_info=True)
                raise e
            
            # 5. ëª¨ë¸ ì €ì¥
            print("\nğŸ”„ 5ë‹¨ê³„: ëª¨ë¸ ì €ì¥...")
            await self.model_loader.save_trained_model(trainer, tokenizer)
            
            # í•™ìŠµ ê²°ê³¼ ì¶œë ¥
            print("\n" + "=" * 60)
            print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
            print("=" * 60)
            print(f"ğŸ“Š í•™ìŠµ ê²°ê³¼:")
            print(f"   - ìµœì¢… ì†ì‹¤: {train_result.training_loss:.4f}")
            print(f"   - í•™ìŠµ ì‹œê°„: {train_result.metrics.get('train_runtime', 0):.2f}ì´ˆ")
            print(f"   - ì´ˆë‹¹ ìƒ˜í”Œ: {train_result.metrics.get('train_samples_per_second', 0):.2f}")
            print(f"   - ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: ./outputs/")
            print("=" * 60)
            
            return train_result
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            print(f"\nâŒ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            raise e

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="ë‰´ìŠ¤ ìš”ì•½ ëª¨ë¸ QLoRA í•™ìŠµ")
    
    parser.add_argument("--epochs", type=int, default=15, help="í•™ìŠµ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 15)")
    parser.add_argument("--batch_size", type=int, default=1, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 1)")
    parser.add_argument("--learning_rate", type=float, default=2e-4, help="í•™ìŠµë¥  (ê¸°ë³¸ê°’: 2e-4)")
    parser.add_argument("--max_seq_length", type=int, default=512, help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 512)")
    parser.add_argument("--data_path", type=str, default="./data/final_input_output_dataset_filtered.csv", 
                       help="ë°ì´í„°ì…‹ ê²½ë¡œ")
    
    return parser.parse_args()

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
        args = parse_arguments()
        
        # ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(args.data_path):
            print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.data_path}")
            print("ğŸ’¡ data/ í´ë”ì— final_input_output_dataset_filtered.csv íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return
        
        # í•™ìŠµ ì„¤ì •
        config = {
            "epochs": args.epochs,
            "batch_size": args.batch_size,
            "learning_rate": args.learning_rate,
            "max_seq_length": args.max_seq_length,
            "data_path": args.data_path
        }
        
        # í•™ìŠµ ì‹¤í–‰
        trainer = DirectTrainer()
        await trainer.run_training(config)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"í•™ìŠµ ì˜¤ë¥˜: {str(e)}", exc_info=True)

if __name__ == "__main__":
    print("ğŸš€ ë‰´ìŠ¤ ìš”ì•½ ëª¨ë¸ ì§ì ‘ í•™ìŠµ ì‹œì‘...")
    print("ğŸ’¡ ì‚¬ìš©ë²•: python train.py --epochs 15 --batch_size 1 --learning_rate 2e-4")
    print()
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main()) 