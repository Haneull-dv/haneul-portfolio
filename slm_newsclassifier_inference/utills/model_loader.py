"""
GPU ìµœì í™” ë‰´ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ ë¡œë”
RTX 2080 + BERT ìµœì í™”
"""
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import os
import logging

logger = logging.getLogger(__name__)

class ModelLoader:
    def __init__(self, model_path=None):
        # GPU ê°•ì œ ì„¤ì • (RTX 2080)
        if not torch.cuda.is_available():
            logger.warning("âš ï¸ CUDA GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPUë¡œ fallbackí•©ë‹ˆë‹¤.")
            self.device = "cpu"
        else:
            self.device = "cuda:0"
            torch.cuda.set_device(0)
            logger.info(f"ğŸš€ GPU í™˜ê²½ ì´ˆê¸°í™”: {torch.cuda.get_device_name(0)}")
        
        if model_path is None:
            # í•™ìŠµëœ BERT ë¶„ë¥˜ ëª¨ë¸ ê²½ë¡œ (GPU ìµœì í™”)
            trained_model_path = "/app/slm_newsclassifier_training/outputs/model"
            if os.path.exists(trained_model_path):
                self.model_path = trained_model_path
                logger.info(f"ğŸ“ í•™ìŠµëœ ëª¨ë¸ ë°œê²¬: {trained_model_path}")
            else:
                # í´ë°±: ë² ì´ìŠ¤ ëª¨ë¸
                self.model_path = "klue/bert-base"
                logger.warning(f"âš ï¸ í•™ìŠµëœ ëª¨ë¸ ì—†ìŒ, ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©: {self.model_path}")
        else:
            self.model_path = model_path
        
        self.tokenizer = None
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """GPU ìµœì í™” BERT ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info(f"ğŸ”„ GPU ê¸°ë°˜ BERT ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_path}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device.startswith("cuda"):
                torch.cuda.empty_cache()
            
            # í† í¬ë‚˜ì´ì € ë¡œë”© (ì˜¤í”„ë¼ì¸ ìš°ì„  + ì•ˆì „ ëª¨ë“œ)
            try:
                # 1ì°¨ ì‹œë„: ì˜¤í”„ë¼ì¸ ëª¨ë“œ
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    local_files_only=True
                )
                logger.info(f"âœ… ì˜¤í”„ë¼ì¸ í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ")
            except:
                # 2ì°¨ ì‹œë„: ì˜¨ë¼ì¸ ëª¨ë“œ (ì¸ì¦ ì—†ì´)
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    local_files_only=False,
                    trust_remote_code=True
                )
                logger.info(f"âœ… ì˜¨ë¼ì¸ í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ")
            
            # GPU ìµœì í™” BERT ëª¨ë¸ ë¡œë”©
            if self.device.startswith("cuda"):
                try:
                    # 1ì°¨ ì‹œë„: ì˜¤í”„ë¼ì¸ ëª¨ë“œ
                    self.model = AutoModelForSequenceClassification.from_pretrained(
                        self.model_path,
                        torch_dtype=torch.float16,
                        local_files_only=True
                    ).to(self.device)
                    logger.info(f"âœ… ì˜¤í”„ë¼ì¸ GPU ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                except:
                    # 2ì°¨ ì‹œë„: ì˜¨ë¼ì¸ ëª¨ë“œ (ì¸ì¦ ì—†ì´)
                    self.model = AutoModelForSequenceClassification.from_pretrained(
                        self.model_path,
                        torch_dtype=torch.float16,
                        local_files_only=False,
                        trust_remote_code=True
                    ).to(self.device)
                    logger.info(f"âœ… ì˜¨ë¼ì¸ GPU ëª¨ë¸ ë¡œë”© ì„±ê³µ")
                
                # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                memory_allocated = torch.cuda.memory_allocated(0) / 1e9
                logger.info(f"ğŸ¯ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_allocated:.2f}GB")
            else:
                # CPU fallback
                try:
                    self.model = AutoModelForSequenceClassification.from_pretrained(
                        self.model_path,
                        local_files_only=True
                    )
                except:
                    self.model = AutoModelForSequenceClassification.from_pretrained(
                        self.model_path,
                        local_files_only=False,
                        trust_remote_code=True
                    )
            
            # ì¶”ë¡  ëª¨ë“œ ì„¤ì •
            self.model.eval()
            
            logger.info(f"âœ… BERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_path} ({self.device})")
            
        except Exception as e:
            logger.error(f"âŒ BERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e
    
    def get_model(self):
        """ëª¨ë¸ ë°˜í™˜"""
        return self.model
    
    def get_tokenizer(self):
        """í† í¬ë‚˜ì´ì € ë°˜í™˜"""
        return self.tokenizer
    
    def get_device(self):
        """ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self.device
    
    def predict(self, texts, max_length=512):
        """GPU ìµœì í™” ë°°ì¹˜ ì˜ˆì¸¡"""
        if not isinstance(texts, list):
            texts = [texts]
        
        try:
            # GPU í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                texts,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=max_length
            )
            
            # GPUë¡œ ì´ë™
            if self.device.startswith("cuda"):
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # GPU ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                
                # CPUë¡œ ê²°ê³¼ ì´ë™
                predictions = predictions.cpu()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device.startswith("cuda"):
                del inputs, outputs
                torch.cuda.empty_cache()
            
            return predictions
            
        except Exception as e:
            logger.error(f"âŒ GPU ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì—ëŸ¬ ì‹œì—ë„)
            if self.device.startswith("cuda"):
                torch.cuda.empty_cache()
            raise e
    
    def get_gpu_memory_info(self):
        """GPU ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
        if self.device.startswith("cuda"):
            return {
                "allocated": torch.cuda.memory_allocated(0) / 1e9,
                "reserved": torch.cuda.memory_reserved(0) / 1e9,
                "total": torch.cuda.get_device_properties(0).total_memory / 1e9
            }
        return {"message": "CPU ëª¨ë“œì—ì„œ ì‹¤í–‰ ì¤‘"} 